import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, DBSCAN, OPTICS
from sklearn.mixture import GaussianMixture
from sklearn_extra.cluster import KMedoids
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
import umap
import warnings
warnings.filterwarnings('ignore')

# Configuration de la page
st.set_page_config(
    page_title="AI Clustering & Insight Dashboard",
    page_icon="üî¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√©
st.markdown("""
    <style>
    .main {
        background-color: #f5f7fa;
    }
    .stMetric {
        background-color: white;
        padding: 15px;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    h1 {
        color: #1f77b4;
        font-weight: 700;
    }
    h2, h3 {
        color: #2c3e50;
    }
    .reportview-container .main .block-container {
        padding-top: 2rem;
    }
    </style>
    """, unsafe_allow_html=True)

# Fonction pour pr√©traiter les donn√©es
@st.cache_data
def preprocess_data(df):
    """Pr√©traite les donn√©es en g√©rant les valeurs manquantes et en encodant les variables cat√©gorielles"""
    df_processed = df.copy()
    
    # S√©paration des colonnes num√©riques et cat√©gorielles
    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()
    categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()
    
    # Gestion des valeurs manquantes pour les colonnes num√©riques
    for col in numeric_cols:
        df_processed[col].fillna(df_processed[col].median(), inplace=True)
    
    # Gestion des valeurs manquantes pour les colonnes cat√©gorielles
    for col in categorical_cols:
        df_processed[col].fillna(df_processed[col].mode()[0] if not df_processed[col].mode().empty else 'Unknown', inplace=True)
    
    # Encodage des variables cat√©gorielles
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        df_processed[col + '_encoded'] = le.fit_transform(df_processed[col].astype(str))
        label_encoders[col] = le
    
    return df_processed, numeric_cols, categorical_cols, label_encoders

# Fonction pour r√©duire la dimensionnalit√©
@st.cache_data
def reduce_dimensions(data, method='PCA', n_components=2, perplexity=30, n_neighbors=15, min_dist=0.1):
    """R√©duit la dimensionnalit√© des donn√©es"""
    if method == 'PCA':
        reducer = PCA(n_components=n_components, random_state=42)
        reduced_data = reducer.fit_transform(data)
        variance_explained = reducer.explained_variance_ratio_
        return reduced_data, variance_explained
    
    elif method == 't-SNE':
        reducer = TSNE(n_components=n_components, perplexity=perplexity, random_state=42, n_iter=1000)
        reduced_data = reducer.fit_transform(data)
        return reduced_data, None
    
    elif method == 'UMAP':
        reducer = umap.UMAP(n_components=n_components, n_neighbors=n_neighbors, min_dist=min_dist, random_state=42)
        reduced_data = reducer.fit_transform(data)
        return reduced_data, None
    
    return data, None

# Fonction pour appliquer le clustering
@st.cache_data
def apply_clustering(data, algorithm='K-Means', n_clusters=3, eps=0.5, min_samples=5, max_eps=np.inf):
    """Applique l'algorithme de clustering s√©lectionn√©"""
    if algorithm == 'K-Means':
        model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        labels = model.fit_predict(data)
        centers = model.cluster_centers_
        return labels, centers
    
    elif algorithm == 'DBSCAN':
        model = DBSCAN(eps=eps, min_samples=min_samples)
        labels = model.fit_predict(data)
        # DBSCAN n'a pas de centres pr√©d√©finis, on calcule les centro√Ødes
        unique_labels = set(labels)
        centers = []
        for label in unique_labels:
            if label != -1:  # Ignorer le bruit
                cluster_points = data[labels == label]
                centers.append(cluster_points.mean(axis=0))
        centers = np.array(centers) if centers else None
        return labels, centers
    
    elif algorithm == 'GMM':
        model = GaussianMixture(n_components=n_clusters, random_state=42)
        labels = model.fit_predict(data)
        centers = model.means_
        return labels, centers
    
    elif algorithm == 'OPTICS':
        model = OPTICS(min_samples=min_samples, max_eps=max_eps)
        labels = model.fit_predict(data)
        unique_labels = set(labels)
        centers = []
        for label in unique_labels:
            if label != -1:
                cluster_points = data[labels == label]
                centers.append(cluster_points.mean(axis=0))
        centers = np.array(centers) if centers else None
        return labels, centers
    
    elif algorithm == 'K-Medoids':
        model = KMedoids(n_clusters=n_clusters, random_state=42)
        labels = model.fit_predict(data)
        centers = model.cluster_centers_
        return labels, centers
    
    return None, None

# Fonction pour calculer les m√©triques de clustering
def calculate_metrics(data, labels):
    """Calcule les m√©triques de qualit√© du clustering"""
    # Filtrer les points de bruit (label -1) pour les algorithmes bas√©s sur la densit√©
    mask = labels != -1
    if mask.sum() < 2:
        return None, None, None, 0
    
    filtered_data = data[mask]
    filtered_labels = labels[mask]
    
    # V√©rifier qu'il y a au moins 2 clusters
    n_clusters = len(set(filtered_labels))
    if n_clusters < 2:
        return None, None, None, n_clusters
    
    try:
        silhouette = silhouette_score(filtered_data, filtered_labels)
        davies_bouldin = davies_bouldin_score(filtered_data, filtered_labels)
        calinski_harabasz = calinski_harabasz_score(filtered_data, filtered_labels)
        return silhouette, davies_bouldin, calinski_harabasz, n_clusters
    except:
        return None, None, None, n_clusters

# Fonction pour g√©n√©rer l'analyse IA
def generate_ai_insights(df, labels, numeric_cols):
    """G√©n√®re des insights automatiques sur les clusters"""
    insights = []
    unique_labels = sorted(set(labels))
    
    for label in unique_labels:
        if label == -1:
            insights.append({
                'cluster': 'Bruit/Outliers',
                'size': sum(labels == label),
                'description': 'Points consid√©r√©s comme du bruit ou des outliers par l\'algorithme'
            })
            continue
        
        cluster_data = df[labels == label][numeric_cols]
        cluster_size = len(cluster_data)
        
        # Calculer les statistiques du cluster
        stats = {}
        for col in numeric_cols[:5]:  # Limiter aux 5 premi√®res colonnes pour la lisibilit√©
            mean_val = cluster_data[col].mean()
            overall_mean = df[col].mean()
            diff_pct = ((mean_val - overall_mean) / overall_mean) * 100 if overall_mean != 0 else 0
            stats[col] = {
                'mean': mean_val,
                'diff_pct': diff_pct
            }
        
        # G√©n√©rer une description
        description = f"Cluster de {cluster_size} √©l√©ments. "
        significant_features = []
        for col, stat in stats.items():
            if abs(stat['diff_pct']) > 20:
                direction = "sup√©rieur" if stat['diff_pct'] > 0 else "inf√©rieur"
                significant_features.append(f"{col} {direction} de {abs(stat['diff_pct']):.1f}%")
        
        if significant_features:
            description += "Caract√©ristiques: " + ", ".join(significant_features[:3])
        else:
            description += "Profil proche de la moyenne g√©n√©rale"
        
        insights.append({
            'cluster': f'Cluster {label}',
            'size': cluster_size,
            'description': description
        })
    
    return insights

# Interface principale
def main():
    st.title("üî¨ AI Clustering & Insight Dashboard")
    st.markdown("### Explorez, segmentez et visualisez vos donn√©es avec l'IA")
    
    # Sidebar pour le chargement des donn√©es
    st.sidebar.header("üìä Chargement des donn√©es")
    
    # Option pour charger un fichier
    uploaded_file = st.sidebar.file_uploader(
        "Choisir un fichier CSV",
        type=['csv'],
        help="T√©l√©chargez votre fichier de donn√©es au format CSV"
    )
    
    if uploaded_file is not None:
        # Charger les donn√©es
        df = pd.read_csv(uploaded_file)
        
        st.sidebar.success(f"‚úÖ Fichier charg√©: {uploaded_file.name}")
        st.sidebar.metric("Nombre de lignes", df.shape[0])
        st.sidebar.metric("Nombre de colonnes", df.shape[1])
        
        # Aper√ßu des donn√©es
        with st.expander("üìã Aper√ßu des donn√©es", expanded=False):
            st.dataframe(df.head(10), use_container_width=True)
            
            col1, col2 = st.columns(2)
            with col1:
                st.write("**Types de donn√©es:**")
                st.write(df.dtypes.value_counts())
            with col2:
                st.write("**Valeurs manquantes:**")
                missing = df.isnull().sum()
                if missing.sum() > 0:
                    st.write(missing[missing > 0])
                else:
                    st.write("Aucune valeur manquante")
        
        # Pr√©traitement
        df_processed, numeric_cols, categorical_cols, label_encoders = preprocess_data(df)
        
        # S√©lection des colonnes pour le clustering
        st.sidebar.header("üéØ Configuration du clustering")
        
        available_cols = numeric_cols.copy()
        for col in categorical_cols:
            available_cols.append(col + '_encoded')
        
        selected_cols = st.sidebar.multiselect(
            "S√©lectionner les colonnes pour le clustering",
            available_cols,
            default=numeric_cols[:min(5, len(numeric_cols))],
            help="Choisissez les variables √† utiliser pour le clustering"
        )
        
        if len(selected_cols) < 2:
            st.warning("‚ö†Ô∏è Veuillez s√©lectionner au moins 2 colonnes pour le clustering")
            return
        
        # Pr√©parer les donn√©es pour le clustering
        X = df_processed[selected_cols].values
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Configuration de la r√©duction de dimensionnalit√©
        st.sidebar.header("üìâ R√©duction de dimensionnalit√©")
        reduction_method = st.sidebar.selectbox(
            "M√©thode de r√©duction",
            ['PCA', 't-SNE', 'UMAP'],
            help="Choisissez la m√©thode pour visualiser les donn√©es en 2D/3D"
        )
        
        n_components = st.sidebar.selectbox("Nombre de composantes", [2, 3], index=0)
        
        # Param√®tres sp√©cifiques selon la m√©thode
        if reduction_method == 't-SNE':
            perplexity = st.sidebar.slider("Perplexit√©", 5, 50, 30, help="√âquilibre entre aspects locaux et globaux")
        else:
            perplexity = 30
        
        if reduction_method == 'UMAP':
            n_neighbors = st.sidebar.slider("Nombre de voisins", 2, 100, 15)
            min_dist = st.sidebar.slider("Distance minimale", 0.0, 1.0, 0.1, 0.05)
        else:
            n_neighbors = 15
            min_dist = 0.1
        
        # Configuration du clustering
        st.sidebar.header("üé≤ Algorithme de clustering")
        clustering_algorithm = st.sidebar.selectbox(
            "Algorithme",
            ['K-Means', 'DBSCAN', 'GMM', 'OPTICS', 'K-Medoids'],
            help="S√©lectionnez l'algorithme de clustering √† utiliser"
        )
        
        # Param√®tres sp√©cifiques selon l'algorithme
        if clustering_algorithm in ['K-Means', 'GMM', 'K-Medoids']:
            n_clusters = st.sidebar.slider("Nombre de clusters (k)", 2, 10, 3)
        else:
            n_clusters = 3
        
        if clustering_algorithm in ['DBSCAN', 'OPTICS']:
            eps = st.sidebar.slider("Epsilon (Œµ)", 0.1, 5.0, 0.5, 0.1, help="Distance maximale entre deux points")
            min_samples = st.sidebar.slider("Min samples", 2, 20, 5, help="Nombre minimum de points pour former un cluster")
            if clustering_algorithm == 'OPTICS':
                max_eps = st.sidebar.slider("Max epsilon", 1.0, 10.0, 5.0, 0.5)
            else:
                max_eps = np.inf
        else:
            eps = 0.5
            min_samples = 5
            max_eps = np.inf
        
        # Bouton pour lancer l'analyse
        if st.sidebar.button("üöÄ Lancer l'analyse", type="primary", use_container_width=True):
            with st.spinner("üîÑ Analyse en cours..."):
                
                # R√©duction de dimensionnalit√©
                X_reduced, variance_explained = reduce_dimensions(
                    X_scaled, reduction_method, n_components, perplexity, n_neighbors, min_dist
                )
                
                # Clustering
                labels, centers = apply_clustering(
                    X_scaled, clustering_algorithm, n_clusters, eps, min_samples, max_eps
                )
                
                # Calculer les m√©triques
                silhouette, davies_bouldin, calinski, n_found_clusters = calculate_metrics(X_scaled, labels)
                
                # Stocker dans session state
                st.session_state['analysis_done'] = True
                st.session_state['X_reduced'] = X_reduced
                st.session_state['labels'] = labels
                st.session_state['centers'] = centers
                st.session_state['variance_explained'] = variance_explained
                st.session_state['silhouette'] = silhouette
                st.session_state['davies_bouldin'] = davies_bouldin
                st.session_state['calinski'] = calinski
                st.session_state['n_found_clusters'] = n_found_clusters
                st.session_state['reduction_method'] = reduction_method
                st.session_state['clustering_algorithm'] = clustering_algorithm
                st.session_state['df_processed'] = df_processed
                st.session_state['numeric_cols'] = numeric_cols
                
            st.success("‚úÖ Analyse termin√©e!")
            st.rerun()
        
        # Affichage des r√©sultats
        if st.session_state.get('analysis_done', False):
            X_reduced = st.session_state['X_reduced']
            labels = st.session_state['labels']
            centers = st.session_state['centers']
            variance_explained = st.session_state['variance_explained']
            silhouette = st.session_state['silhouette']
            davies_bouldin = st.session_state['davies_bouldin']
            calinski = st.session_state['calinski']
            n_found_clusters = st.session_state['n_found_clusters']
            reduction_method = st.session_state['reduction_method']
            clustering_algorithm = st.session_state['clustering_algorithm']
            
            # M√©triques en haut
            st.header("üìä M√©triques de performance")
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("Nombre de clusters", n_found_clusters)
            
            with col2:
                if silhouette is not None:
                    st.metric("Silhouette Score", f"{silhouette:.3f}", 
                             help="Entre -1 et 1. Plus c'est proche de 1, meilleur c'est")
                else:
                    st.metric("Silhouette Score", "N/A")
            
            with col3:
                if davies_bouldin is not None:
                    st.metric("Davies-Bouldin Index", f"{davies_bouldin:.3f}",
                             help="Plus c'est proche de 0, meilleur c'est")
                else:
                    st.metric("Davies-Bouldin Index", "N/A")
            
            with col4:
                if calinski is not None:
                    st.metric("Calinski-Harabasz", f"{calinski:.0f}",
                             help="Plus c'est √©lev√©, meilleur c'est")
                else:
                    st.metric("Calinski-Harabasz", "N/A")
            
            # Visualisations
            st.header("üìà Visualisations")
            
            tab1, tab2, tab3, tab4 = st.tabs(["üó∫Ô∏è Scatter Plot", "üìä Distribution", "üéØ Profils", "ü§ñ Insights IA"])
            
            with tab1:
                # Scatter plot 2D ou 3D
                if n_components == 2:
                    fig = px.scatter(
                        x=X_reduced[:, 0],
                        y=X_reduced[:, 1],
                        color=labels.astype(str),
                        title=f"Visualisation des clusters - {reduction_method}",
                        labels={'x': 'Composante 1', 'y': 'Composante 2', 'color': 'Cluster'},
                        color_discrete_sequence=px.colors.qualitative.Bold,
                        width=800,
                        height=600
                    )
                    
                    # Ajouter les centres si disponibles
                    if centers is not None and reduction_method == 'PCA':
                        centers_reduced, _ = reduce_dimensions(centers, 'PCA', n_components)
                        fig.add_trace(go.Scatter(
                            x=centers_reduced[:, 0],
                            y=centers_reduced[:, 1],
                            mode='markers',
                            marker=dict(size=20, symbol='x', color='black', line=dict(width=2)),
                            name='Centres',
                            showlegend=True
                        ))
                    
                    if variance_explained is not None:
                        fig.add_annotation(
                            text=f"Variance expliqu√©e: {sum(variance_explained)*100:.1f}%",
                            xref="paper", yref="paper",
                            x=0.02, y=0.98,
                            showarrow=False,
                            bgcolor="white",
                            bordercolor="black",
                            borderwidth=1
                        )
                    
                    fig.update_layout(
                        plot_bgcolor='white',
                        paper_bgcolor='white',
                        font=dict(size=12)
                    )
                    st.plotly_chart(fig, use_container_width=True)
                
                else:  # 3D
                    fig = px.scatter_3d(
                        x=X_reduced[:, 0],
                        y=X_reduced[:, 1],
                        z=X_reduced[:, 2],
                        color=labels.astype(str),
                        title=f"Visualisation 3D des clusters - {reduction_method}",
                        labels={'x': 'Composante 1', 'y': 'Composante 2', 'z': 'Composante 3', 'color': 'Cluster'},
                        color_discrete_sequence=px.colors.qualitative.Bold,
                        width=800,
                        height=700
                    )
                    
                    if variance_explained is not None:
                        fig.add_annotation(
                            text=f"Variance expliqu√©e: {sum(variance_explained)*100:.1f}%",
                            xref="paper", yref="paper",
                            x=0.02, y=0.98,
                            showarrow=False
                        )
                    
                    st.plotly_chart(fig, use_container_width=True)
            
            with tab2:
                # Distribution des clusters
                unique_labels, counts = np.unique(labels, return_counts=True)
                
                fig = px.bar(
                    x=[f"Cluster {l}" if l != -1 else "Bruit" for l in unique_labels],
                    y=counts,
                    title="Distribution des points par cluster",
                    labels={'x': 'Cluster', 'y': 'Nombre de points'},
                    color=counts,
                    color_continuous_scale='Blues'
                )
                fig.update_layout(showlegend=False, plot_bgcolor='white', paper_bgcolor='white')
                st.plotly_chart(fig, use_container_width=True)
                
                # Tableau des tailles
                st.subheader("üìã Taille des clusters")
                cluster_sizes = pd.DataFrame({
                    'Cluster': [f"Cluster {l}" if l != -1 else "Bruit/Outliers" for l in unique_labels],
                    'Nombre de points': counts,
                    'Pourcentage': [f"{(c/len(labels))*100:.1f}%" for c in counts]
                })
                st.dataframe(cluster_sizes, use_container_width=True, hide_index=True)
            
            with tab3:
                # Profils des clusters
                st.subheader("üéØ Profils des clusters")
                
                df_with_clusters = df_processed.copy()
                df_with_clusters['Cluster'] = labels
                
                # Radar chart pour chaque cluster
                unique_labels_sorted = sorted([l for l in set(labels) if l != -1])
                
                if len(numeric_cols) >= 3:
                    selected_features = numeric_cols[:min(6, len(numeric_cols))]
                    
                    for label in unique_labels_sorted[:4]:  # Limiter √† 4 clusters pour la lisibilit√©
                        cluster_data = df_with_clusters[df_with_clusters['Cluster'] == label][selected_features]
                        overall_data = df_processed[selected_features]
                        
                        # Normaliser les valeurs
                        cluster_means = []
                        overall_means = []
                        
                        for col in selected_features:
                            cluster_mean = cluster_data[col].mean()
                            overall_mean = overall_data[col].mean()
                            overall_std = overall_data[col].std()
                            
                            if overall_std > 0:
                                cluster_normalized = (cluster_mean - overall_mean) / overall_std
                            else:
                                cluster_normalized = 0
                            
                            cluster_means.append(cluster_normalized)
                            overall_means.append(0)  # Moyenne g√©n√©rale = 0 apr√®s normalisation
                        
                        fig = go.Figure()
                        
                        fig.add_trace(go.Scatterpolar(
                            r=cluster_means + [cluster_means[0]],
                            theta=selected_features + [selected_features[0]],
                            fill='toself',
                            name=f'Cluster {label}'
                        ))
                        
                        fig.add_trace(go.Scatterpolar(
                            r=overall_means + [overall_means[0]],
                            theta=selected_features + [selected_features[0]],
                            fill='toself',
                            name='Moyenne g√©n√©rale',
                            line=dict(dash='dash')
                        ))
                        
                        fig.update_layout(
                            polar=dict(
                                radialaxis=dict(visible=True, range=[-3, 3])
                            ),
                            showlegend=True,
                            title=f"Profil du Cluster {label} (scores standardis√©s)",
                            height=400
                        )
                        
                        st.plotly_chart(fig, use_container_width=True)
                
                # Heatmap des moyennes par cluster
                st.subheader("üî• Heatmap des caract√©ristiques")
                
                cluster_profiles = []
                for label in unique_labels_sorted:
                    profile = df_with_clusters[df_with_clusters['Cluster'] == label][numeric_cols[:10]].mean()
                    cluster_profiles.append(profile)
                
                if cluster_profiles:
                    heatmap_data = pd.DataFrame(cluster_profiles)
                    heatmap_data.index = [f'Cluster {i}' for i in unique_labels_sorted]
                    
                    # Normaliser pour la heatmap
                    heatmap_normalized = (heatmap_data - heatmap_data.mean()) / heatmap_data.std()
                    
                    fig = px.imshow(
                        heatmap_normalized.T,
                        labels=dict(x="Cluster", y="Caract√©ristique", color="Valeur standardis√©e"),
                        x=heatmap_normalized.index,
                        y=heatmap_normalized.columns,
                        color_continuous_scale='RdBu_r',
                        aspect='auto',
                        title="Profil des clusters (valeurs standardis√©es)"
                    )
                    fig.update_layout(height=500)
                    st.plotly_chart(fig, use_container_width=True)
            
            with tab4:
                # Insights IA
                st.subheader("ü§ñ Analyse automatique par IA")
                
                insights = generate_ai_insights(df_processed, labels, numeric_cols)
                
                for insight in insights:
                    with st.container():
                        st.markdown(f"### {insight['cluster']}")
                        col1, col2 = st.columns([1, 3])
                        with col1:
                            st.metric("Taille", insight['size'])
                        with col2:
                            st.info(insight['description'])
                        st.divider()
                
                # R√©sum√© global
                st.subheader("üìù R√©sum√© de l'analyse")
                
                quality_text = ""
                if silhouette is not None:
                    if silhouette > 0.5:
                        quality_text = "**Excellente** s√©paration des clusters"
                    elif silhouette > 0.3:
                        quality_text = "**Bonne** s√©paration des clusters"
                    elif silhouette > 0:
                        quality_text = "**Faible** s√©paration des clusters"
                    else:
                        quality_text = "**Mauvaise** s√©paration des clusters"
                
                # Format metrics safely
                sil_text = f"{silhouette:.3f}" if silhouette is not None else "N/A"
                db_text = f"{davies_bouldin:.3f}" if davies_bouldin is not None else "N/A"
                
                st.markdown(f"""
                **Configuration:**
                - Algorithme: {clustering_algorithm}
                - M√©thode de r√©duction: {reduction_method}
                - Nombre de clusters identifi√©s: {n_found_clusters}
                
                **Qualit√© du clustering:**
                - {quality_text}
                - Silhouette Score: {sil_text}
                - Davies-Bouldin Index: {db_text}
                
                **Recommandations:**
                """)
                
                # Ajouter les recommandations sans backslash dans f-string
                if silhouette and silhouette > 0.5:
                    st.markdown("- ‚úÖ Les clusters sont bien d√©finis et distincts.")
                else:
                    st.markdown("- ‚ö†Ô∏è Envisagez de tester diff√©rents param√®tres ou algorithmes pour am√©liorer la s√©paration.")
                
                if davies_bouldin and davies_bouldin < 1.5:
                    st.markdown("- ‚úÖ Le nombre de clusters semble appropri√©.")
                else:
                    st.markdown("- üí° Essayez diff√©rentes valeurs de k ou d'epsilon pour optimiser le clustering.")
                
                # T√©l√©charger les r√©sultats
                st.subheader("üíæ T√©l√©charger les r√©sultats")
                
                results_df = df_processed.copy()
                results_df['Cluster'] = labels
                results_df['Component_1'] = X_reduced[:, 0]
                results_df['Component_2'] = X_reduced[:, 1]
                
                csv = results_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üì• T√©l√©charger les donn√©es avec clusters",
                    data=csv,
                    file_name='clustering_results.csv',
                    mime='text/csv',
                    use_container_width=True
                )
    
    else:
        # Message d'accueil
        st.info("üëà Commencez par charger un fichier CSV dans la barre lat√©rale")
        
        st.markdown("""
        ### üöÄ Fonctionnalit√©s principales
        
        - **Algorithmes de clustering:** K-Means, DBSCAN, GMM, OPTICS, K-Medoids
        - **R√©duction de dimensionnalit√©:** PCA, t-SNE, UMAP
        - **M√©triques de qualit√©:** Silhouette, Davies-Bouldin, Calinski-Harabasz
        - **Visualisations interactives:** Scatter plots 2D/3D, Radar charts, Heatmaps
        - **Insights IA:** Analyse automatique et recommandations
        
        ### üìä Types de donn√©es support√©s
        
        - Donn√©es clients, transactions, produits
        - Donn√©es RH, employ√©s
        - Donn√©es de ventes, marketing
        - Tout dataset tabulaire au format CSV
        
        ### üéØ Comment utiliser
        
        1. Chargez votre fichier CSV
        2. S√©lectionnez les colonnes √† analyser
        3. Choisissez la m√©thode de r√©duction de dimensionnalit√©
        4. S√©lectionnez l'algorithme de clustering
        5. Ajustez les hyperparam√®tres
        6. Lancez l'analyse et explorez les r√©sultats
        """)

if __name__ == "__main__":
    main()
