{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203fbe9e",
   "metadata": {},
   "source": [
    "# ğŸ”¬ AI Clustering & Insight Dashboard\n",
    "\n",
    "## Challenge 2 : Clustering Intelligent avec Visualisations Interactives\n",
    "\n",
    "Ce notebook propose une analyse complÃ¨te de clustering avec :\n",
    "- **Algorithmes** : K-Means, DBSCAN, GMM, OPTICS, K-Medoids\n",
    "- **RÃ©duction de dimensionnalitÃ©** : PCA, t-SNE, UMAP\n",
    "- **MÃ©triques** : Silhouette, Davies-Bouldin, Calinski-Harabasz\n",
    "- **Visualisations** : Plotly interactif, Radar charts, Heatmaps\n",
    "- **IA** : Analyse automatique des profils de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef04337",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Installation des bibliothÃ¨ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e29714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des bibliothÃ¨ques nÃ©cessaires\n",
    "!pip install umap-learn scikit-learn-extra plotly pandas numpy matplotlib seaborn -q\n",
    "\n",
    "print(\"âœ… Installation terminÃ©e!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da850a",
   "metadata": {},
   "source": [
    "## ğŸ“š Importation des bibliothÃ¨ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340cbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotly pour visualisations interactives\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# RÃ©duction de dimensionnalitÃ©\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# Algorithmes de clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "# MÃ©triques d'Ã©valuation\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics import silhouette_samples\n",
    "\n",
    "# Pour charger des fichiers depuis Google Drive\n",
    "from google.colab import files\n",
    "\n",
    "print(\"âœ… BibliothÃ¨ques importÃ©es avec succÃ¨s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f4621",
   "metadata": {},
   "source": [
    "## ğŸ“Š Chargement des donnÃ©es\n",
    "\n",
    "Vous pouvez charger vos donnÃ©es de 3 faÃ§ons :\n",
    "1. **Upload direct** depuis votre ordinateur\n",
    "2. **Google Drive** (monter votre drive)\n",
    "3. **URL** (lien direct vers un CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Upload depuis votre ordinateur\n",
    "print(\"ğŸ“ SÃ©lectionnez votre fichier CSV...\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Charger le premier fichier uploadÃ©\n",
    "filename = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(filename)\n",
    "\n",
    "print(f\"\\nâœ… Fichier chargÃ©: {filename}\")\n",
    "print(f\"ğŸ“ Dimensions: {df.shape[0]} lignes Ã— {df.shape[1]} colonnes\")\n",
    "print(\"\\nğŸ“‹ AperÃ§u des donnÃ©es:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2: Depuis Google Drive (dÃ©commentez si nÃ©cessaire)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# df = pd.read_csv('/content/drive/MyDrive/votre_fichier.csv')\n",
    "\n",
    "# Option 3: Depuis une URL (dÃ©commentez et modifiez l'URL)\n",
    "# url = 'https://example.com/data.csv'\n",
    "# df = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8263f9",
   "metadata": {},
   "source": [
    "## ğŸ” Exploration des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd03283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations gÃ©nÃ©rales\n",
    "print(\"ğŸ“Š INFORMATIONS SUR LE DATASET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Nombre de lignes: {df.shape[0]}\")\n",
    "print(f\"Nombre de colonnes: {df.shape[1]}\")\n",
    "print(f\"\\nMÃ©moire utilisÃ©e: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nğŸ“‹ TYPES DE DONNÃ‰ES\")\n",
    "print(\"=\" * 50)\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nâš ï¸ VALEURS MANQUANTES\")\n",
    "print(\"=\" * 50)\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Colonnes': missing.index,\n",
    "    'Valeurs manquantes': missing.values,\n",
    "    'Pourcentage': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Valeurs manquantes'] > 0].sort_values('Valeurs manquantes', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    display(missing_df)\n",
    "else:\n",
    "    print(\"âœ… Aucune valeur manquante!\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ STATISTIQUES DESCRIPTIVES\")\n",
    "print(\"=\" * 50)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86fb1d9",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ PrÃ©traitement des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddc7db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"\n",
    "    PrÃ©traite les donnÃ©es:\n",
    "    - GÃ¨re les valeurs manquantes\n",
    "    - Encode les variables catÃ©gorielles\n",
    "    - SÃ©pare les colonnes numÃ©riques et catÃ©gorielles\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Identifier les colonnes numÃ©riques et catÃ©gorielles\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"ğŸ“Š Colonnes numÃ©riques: {len(numeric_cols)}\")\n",
    "    print(f\"ğŸ“ Colonnes catÃ©gorielles: {len(categorical_cols)}\")\n",
    "    \n",
    "    # GÃ©rer les valeurs manquantes (colonnes numÃ©riques)\n",
    "    if len(numeric_cols) > 0:\n",
    "        imputer_num = SimpleImputer(strategy='median')\n",
    "        df_processed[numeric_cols] = imputer_num.fit_transform(df_processed[numeric_cols])\n",
    "    \n",
    "    # GÃ©rer les valeurs manquantes (colonnes catÃ©gorielles)\n",
    "    if len(categorical_cols) > 0:\n",
    "        imputer_cat = SimpleImputer(strategy='most_frequent')\n",
    "        df_processed[categorical_cols] = imputer_cat.fit_transform(df_processed[categorical_cols])\n",
    "    \n",
    "    # Encoder les variables catÃ©gorielles\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col + '_encoded'] = le.fit_transform(df_processed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    return df_processed, numeric_cols, categorical_cols, label_encoders\n",
    "\n",
    "# Appliquer le prÃ©traitement\n",
    "df_processed, numeric_cols, categorical_cols, label_encoders = preprocess_data(df)\n",
    "\n",
    "print(\"\\nâœ… PrÃ©traitement terminÃ©!\")\n",
    "print(f\"\\nğŸ“‹ Colonnes numÃ©riques: {numeric_cols[:5]}...\" if len(numeric_cols) > 5 else f\"\\nğŸ“‹ Colonnes numÃ©riques: {numeric_cols}\")\n",
    "if len(categorical_cols) > 0:\n",
    "    print(f\"ğŸ“‹ Colonnes catÃ©gorielles: {categorical_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d23ee8",
   "metadata": {},
   "source": [
    "## ğŸ¯ Configuration de l'analyse\n",
    "\n",
    "SÃ©lectionnez les colonnes Ã  utiliser pour le clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae96e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher toutes les colonnes disponibles\n",
    "available_cols = numeric_cols.copy()\n",
    "for col in categorical_cols:\n",
    "    available_cols.append(col + '_encoded')\n",
    "\n",
    "print(\"ğŸ“Š Colonnes disponibles pour le clustering:\")\n",
    "for i, col in enumerate(available_cols, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "# SÃ©lection automatique (toutes les colonnes numÃ©riques) ou manuelle\n",
    "# Option 1: Utiliser toutes les colonnes numÃ©riques\n",
    "selected_cols = numeric_cols.copy()\n",
    "\n",
    "# Option 2: SÃ©lection manuelle (dÃ©commentez et modifiez)\n",
    "# selected_cols = ['colonne1', 'colonne2', 'colonne3']\n",
    "\n",
    "print(f\"\\nâœ… Colonnes sÃ©lectionnÃ©es ({len(selected_cols)}): {selected_cols}\")\n",
    "\n",
    "# PrÃ©parer les donnÃ©es pour le clustering\n",
    "X = df_processed[selected_cols].values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(f\"\\nğŸ“ Shape des donnÃ©es: {X_scaled.shape}\")\n",
    "print(\"âœ… DonnÃ©es normalisÃ©es et prÃªtes pour le clustering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fe68c",
   "metadata": {},
   "source": [
    "## ğŸ“‰ RÃ©duction de dimensionnalitÃ©\n",
    "\n",
    "Trois mÃ©thodes disponibles: PCA, t-SNE, UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8580833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "# Choisissez la mÃ©thode: 'PCA', 't-SNE', ou 'UMAP'\n",
    "REDUCTION_METHOD = 'PCA'\n",
    "N_COMPONENTS = 2  # 2 pour 2D, 3 pour 3D\n",
    "\n",
    "# ParamÃ¨tres t-SNE\n",
    "PERPLEXITY = 30\n",
    "\n",
    "# ParamÃ¨tres UMAP\n",
    "N_NEIGHBORS = 15\n",
    "MIN_DIST = 0.1\n",
    "# ===================================\n",
    "\n",
    "print(f\"ğŸ”„ RÃ©duction de dimensionnalitÃ© avec {REDUCTION_METHOD}...\")\n",
    "\n",
    "if REDUCTION_METHOD == 'PCA':\n",
    "    reducer = PCA(n_components=N_COMPONENTS, random_state=42)\n",
    "    X_reduced = reducer.fit_transform(X_scaled)\n",
    "    variance_explained = reducer.explained_variance_ratio_\n",
    "    print(f\"âœ… Variance expliquÃ©e: {sum(variance_explained)*100:.2f}%\")\n",
    "    for i, var in enumerate(variance_explained, 1):\n",
    "        print(f\"   - Composante {i}: {var*100:.2f}%\")\n",
    "\n",
    "elif REDUCTION_METHOD == 't-SNE':\n",
    "    reducer = TSNE(n_components=N_COMPONENTS, perplexity=PERPLEXITY, random_state=42, n_iter=1000)\n",
    "    X_reduced = reducer.fit_transform(X_scaled)\n",
    "    print(f\"âœ… t-SNE terminÃ© (perplexity={PERPLEXITY})\")\n",
    "\n",
    "elif REDUCTION_METHOD == 'UMAP':\n",
    "    reducer = umap.UMAP(n_components=N_COMPONENTS, n_neighbors=N_NEIGHBORS, min_dist=MIN_DIST, random_state=42)\n",
    "    X_reduced = reducer.fit_transform(X_scaled)\n",
    "    print(f\"âœ… UMAP terminÃ© (n_neighbors={N_NEIGHBORS}, min_dist={MIN_DIST})\")\n",
    "\n",
    "print(f\"\\nğŸ“ Shape aprÃ¨s rÃ©duction: {X_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd577a",
   "metadata": {},
   "source": [
    "## ğŸ² Clustering - Test de TOUS les algorithmes\n",
    "\n",
    "Nous allons tester automatiquement tous les algorithmes disponibles avec diffÃ©rents paramÃ¨tres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af40b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION POUR TOUS LES ALGORITHMES ==========\n",
    "print(\"ğŸ”„ Test de TOUS les algorithmes de clustering...\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# DÃ©finir tous les algorithmes Ã  tester\n",
    "all_algorithms = {\n",
    "    'K-Means (k=3)': {\n",
    "        'model': KMeans(n_clusters=3, random_state=42, n_init=10),\n",
    "        'name': 'K-Means',\n",
    "        'params': 'k=3'\n",
    "    },\n",
    "    'K-Means (k=4)': {\n",
    "        'model': KMeans(n_clusters=4, random_state=42, n_init=10),\n",
    "        'name': 'K-Means',\n",
    "        'params': 'k=4'\n",
    "    },\n",
    "    'K-Means (k=5)': {\n",
    "        'model': KMeans(n_clusters=5, random_state=42, n_init=10),\n",
    "        'name': 'K-Means',\n",
    "        'params': 'k=5'\n",
    "    },\n",
    "    'DBSCAN (eps=0.3)': {\n",
    "        'model': DBSCAN(eps=0.3, min_samples=5),\n",
    "        'name': 'DBSCAN',\n",
    "        'params': 'eps=0.3, min_samples=5'\n",
    "    },\n",
    "    'DBSCAN (eps=0.5)': {\n",
    "        'model': DBSCAN(eps=0.5, min_samples=5),\n",
    "        'name': 'DBSCAN',\n",
    "        'params': 'eps=0.5, min_samples=5'\n",
    "    },\n",
    "    'DBSCAN (eps=0.7)': {\n",
    "        'model': DBSCAN(eps=0.7, min_samples=5),\n",
    "        'name': 'DBSCAN',\n",
    "        'params': 'eps=0.7, min_samples=5'\n",
    "    },\n",
    "    'GMM (k=3)': {\n",
    "        'model': GaussianMixture(n_components=3, random_state=42),\n",
    "        'name': 'GMM',\n",
    "        'params': 'k=3'\n",
    "    },\n",
    "    'GMM (k=4)': {\n",
    "        'model': GaussianMixture(n_components=4, random_state=42),\n",
    "        'name': 'GMM',\n",
    "        'params': 'k=4'\n",
    "    },\n",
    "    'GMM (k=5)': {\n",
    "        'model': GaussianMixture(n_components=5, random_state=42),\n",
    "        'name': 'GMM',\n",
    "        'params': 'k=5'\n",
    "    },\n",
    "    'OPTICS (min_samples=5)': {\n",
    "        'model': OPTICS(min_samples=5, max_eps=2.0),\n",
    "        'name': 'OPTICS',\n",
    "        'params': 'min_samples=5, max_eps=2.0'\n",
    "    },\n",
    "    'OPTICS (min_samples=10)': {\n",
    "        'model': OPTICS(min_samples=10, max_eps=2.0),\n",
    "        'name': 'OPTICS',\n",
    "        'params': 'min_samples=10, max_eps=2.0'\n",
    "    },\n",
    "    'K-Medoids (k=3)': {\n",
    "        'model': KMedoids(n_clusters=3, random_state=42),\n",
    "        'name': 'K-Medoids',\n",
    "        'params': 'k=3'\n",
    "    },\n",
    "    'K-Medoids (k=4)': {\n",
    "        'model': KMedoids(n_clusters=4, random_state=42),\n",
    "        'name': 'K-Medoids',\n",
    "        'params': 'k=4'\n",
    "    },\n",
    "}\n",
    "\n",
    "# Stocker tous les rÃ©sultats\n",
    "all_results = {}\n",
    "comparison_results = []\n",
    "\n",
    "# Tester chaque algorithme\n",
    "for algo_name, algo_info in all_algorithms.items():\n",
    "    try:\n",
    "        print(f\"\\nğŸ”„ Test de {algo_name}...\")\n",
    "        \n",
    "        # Appliquer le clustering\n",
    "        model = algo_info['model']\n",
    "        test_labels = model.fit_predict(X_scaled)\n",
    "        \n",
    "        # Calculer les centres si disponibles\n",
    "        if hasattr(model, 'cluster_centers_'):\n",
    "            centers = model.cluster_centers_\n",
    "        elif hasattr(model, 'means_'):\n",
    "            centers = model.means_\n",
    "        else:\n",
    "            centers = None\n",
    "        \n",
    "        # Calculer les mÃ©triques\n",
    "        sil, db, ch = calculate_metrics(X_scaled, test_labels)\n",
    "        \n",
    "        # Statistiques des clusters\n",
    "        unique_labels = set(test_labels)\n",
    "        n_clusters = len([l for l in unique_labels if l != -1])\n",
    "        n_noise = sum(test_labels == -1)\n",
    "        \n",
    "        # Stocker les rÃ©sultats\n",
    "        all_results[algo_name] = {\n",
    "            'labels': test_labels,\n",
    "            'centers': centers,\n",
    "            'silhouette': sil,\n",
    "            'davies_bouldin': db,\n",
    "            'calinski': ch,\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_noise': n_noise\n",
    "        }\n",
    "        \n",
    "        # Pour le tableau comparatif\n",
    "        comparison_results.append({\n",
    "            'Algorithme': algo_info['name'],\n",
    "            'ParamÃ¨tres': algo_info['params'],\n",
    "            'N_Clusters': n_clusters,\n",
    "            'N_Outliers': n_noise,\n",
    "            'Silhouette': f\"{sil:.4f}\" if sil else 'N/A',\n",
    "            'Davies-Bouldin': f\"{db:.4f}\" if db else 'N/A',\n",
    "            'Calinski-Harabasz': f\"{ch:.1f}\" if ch else 'N/A'\n",
    "        })\n",
    "        \n",
    "        print(f\"   âœ… TerminÃ©: {n_clusters} clusters, Silhouette={sil:.4f if sil else 'N/A'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Erreur: {str(e)}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… Test de {len(all_results)} algorithmes terminÃ©!\")\n",
    "\n",
    "# CrÃ©er le DataFrame comparatif\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "\n",
    "print(\"\\nğŸ“Š TABLEAU COMPARATIF DE TOUS LES ALGORITHMES\")\n",
    "print(\"=\" * 90)\n",
    "display(comparison_df)\n",
    "\n",
    "# Trouver le meilleur algorithme basÃ© sur le Silhouette Score\n",
    "valid_results = [(name, info) for name, info in all_results.items() if info['silhouette'] is not None]\n",
    "if valid_results:\n",
    "    best_algo = max(valid_results, key=lambda x: x[1]['silhouette'])\n",
    "    best_name = best_algo[0]\n",
    "    best_info = best_algo[1]\n",
    "    \n",
    "    print(f\"\\nğŸ† MEILLEUR ALGORITHME (Silhouette Score):\")\n",
    "    print(f\"   â€¢ Algorithme: {best_name}\")\n",
    "    print(f\"   â€¢ Silhouette Score: {best_info['silhouette']:.4f}\")\n",
    "    print(f\"   â€¢ Davies-Bouldin: {best_info['davies_bouldin']:.4f}\")\n",
    "    print(f\"   â€¢ Nombre de clusters: {best_info['n_clusters']}\")\n",
    "    \n",
    "    # Utiliser le meilleur algorithme pour les visualisations suivantes\n",
    "    labels = best_info['labels']\n",
    "    centers = best_info['centers']\n",
    "    silhouette = best_info['silhouette']\n",
    "    davies_bouldin = best_info['davies_bouldin']\n",
    "    calinski = best_info['calinski']\n",
    "    CLUSTERING_ALGORITHM = best_name\n",
    "    \n",
    "    print(f\"\\nğŸ’¡ Les visualisations suivantes utiliseront: {best_name}\")\n",
    "else:\n",
    "    # Fallback sur le premier algorithme\n",
    "    first_algo = list(all_results.keys())[0]\n",
    "    labels = all_results[first_algo]['labels']\n",
    "    centers = all_results[first_algo]['centers']\n",
    "    silhouette = all_results[first_algo]['silhouette']\n",
    "    davies_bouldin = all_results[first_algo]['davies_bouldin']\n",
    "    calinski = all_results[first_algo]['calinski']\n",
    "    CLUSTERING_ALGORITHM = first_algo\n",
    "    print(f\"\\nâš ï¸ Utilisation de {first_algo} pour les visualisations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8a0f7",
   "metadata": {},
   "source": [
    "## ğŸ“Š Visualisation comparative des algorithmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f75ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique comparatif des Silhouette Scores\n",
    "fig = go.Figure()\n",
    "\n",
    "silhouette_values = []\n",
    "algo_names = []\n",
    "colors = []\n",
    "\n",
    "for result in comparison_results:\n",
    "    try:\n",
    "        sil_val = float(result['Silhouette'])\n",
    "        silhouette_values.append(sil_val)\n",
    "        algo_names.append(f\"{result['Algorithme']}\\n{result['ParamÃ¨tres']}\")\n",
    "        \n",
    "        # Couleur selon la qualitÃ©\n",
    "        if sil_val > 0.5:\n",
    "            colors.append('#2ecc71')  # Vert - Excellent\n",
    "        elif sil_val > 0.3:\n",
    "            colors.append('#f39c12')  # Orange - Bon\n",
    "        else:\n",
    "            colors.append('#e74c3c')  # Rouge - Faible\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    x=algo_names,\n",
    "    y=silhouette_values,\n",
    "    marker_color=colors,\n",
    "    text=[f'{v:.3f}' for v in silhouette_values],\n",
    "    textposition='outside',\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='ğŸ¯ Comparaison des Silhouette Scores - Tous les algorithmes',\n",
    "    xaxis_title='Algorithme',\n",
    "    yaxis_title='Silhouette Score',\n",
    "    plot_bgcolor='white',\n",
    "    height=600,\n",
    "    xaxis={'tickangle': -45},\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.add_hline(y=0.5, line_dash=\"dash\", line_color=\"green\", \n",
    "              annotation_text=\"Excellente sÃ©paration (>0.5)\", \n",
    "              annotation_position=\"right\")\n",
    "fig.add_hline(y=0.3, line_dash=\"dash\", line_color=\"orange\", \n",
    "              annotation_text=\"Bonne sÃ©paration (>0.3)\", \n",
    "              annotation_position=\"right\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Graphique du nombre de clusters trouvÃ©s\n",
    "fig2 = go.Figure()\n",
    "\n",
    "cluster_counts = [result['N_Clusters'] for result in comparison_results]\n",
    "algo_labels = [f\"{result['Algorithme']}\\n{result['ParamÃ¨tres']}\" for result in comparison_results]\n",
    "\n",
    "fig2.add_trace(go.Bar(\n",
    "    x=algo_labels,\n",
    "    y=cluster_counts,\n",
    "    marker_color='#3498db',\n",
    "    text=cluster_counts,\n",
    "    textposition='outside',\n",
    "))\n",
    "\n",
    "fig2.update_layout(\n",
    "    title='ğŸ“Š Nombre de clusters trouvÃ©s par algorithme',\n",
    "    xaxis_title='Algorithme',\n",
    "    yaxis_title='Nombre de clusters',\n",
    "    plot_bgcolor='white',\n",
    "    height=600,\n",
    "    xaxis={'tickangle': -45},\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig2.show()\n",
    "\n",
    "print(\"\\nğŸ“ˆ LÃ©gende des couleurs (Silhouette Score):\")\n",
    "print(\"   ğŸŸ¢ Vert: Excellente sÃ©paration (> 0.5)\")\n",
    "print(\"   ğŸŸ  Orange: Bonne sÃ©paration (0.3 - 0.5)\")\n",
    "print(\"   ğŸ”´ Rouge: Faible sÃ©paration (< 0.3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef180f1c",
   "metadata": {},
   "source": [
    "## ğŸ¨ Visualisation de tous les algorithmes cÃ´te Ã  cÃ´te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b76f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er une grille de visualisations pour comparer visuellement tous les algorithmes\n",
    "# SÃ©lectionner les meilleurs reprÃ©sentants de chaque algorithme\n",
    "\n",
    "best_per_algo = {}\n",
    "for name, info in all_results.items():\n",
    "    algo_base = name.split('(')[0].strip()\n",
    "    if algo_base not in best_per_algo or (info['silhouette'] and \n",
    "        (best_per_algo[algo_base]['silhouette'] is None or \n",
    "         info['silhouette'] > best_per_algo[algo_base]['silhouette'])):\n",
    "        best_per_algo[algo_base] = {**info, 'full_name': name}\n",
    "\n",
    "# CrÃ©er les subplots\n",
    "n_algos = len(best_per_algo)\n",
    "n_cols = 3\n",
    "n_rows = (n_algos + n_cols - 1) // n_cols\n",
    "\n",
    "if N_COMPONENTS == 2:\n",
    "    fig = make_subplots(\n",
    "        rows=n_rows, cols=n_cols,\n",
    "        subplot_titles=[name for name in best_per_algo.keys()],\n",
    "        horizontal_spacing=0.1,\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    for idx, (algo_name, algo_info) in enumerate(best_per_algo.items()):\n",
    "        row = idx // n_cols + 1\n",
    "        col = idx % n_cols + 1\n",
    "        \n",
    "        algo_labels = algo_info['labels']\n",
    "        \n",
    "        # CrÃ©er un scatter pour chaque cluster\n",
    "        for cluster_id in set(algo_labels):\n",
    "            mask = algo_labels == cluster_id\n",
    "            cluster_name = f'Cluster {cluster_id}' if cluster_id != -1 else 'Bruit'\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=X_reduced[mask, 0],\n",
    "                    y=X_reduced[mask, 1],\n",
    "                    mode='markers',\n",
    "                    name=cluster_name,\n",
    "                    marker=dict(size=5, opacity=0.6),\n",
    "                    showlegend=(idx == 0)  # LÃ©gende seulement pour le premier\n",
    "                ),\n",
    "                row=row, col=col\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=400 * n_rows,\n",
    "        title_text=f\"Comparaison visuelle de tous les algorithmes - {REDUCTION_METHOD}\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "print(f\"\\nğŸ“Š {len(best_per_algo)} algorithmes visualisÃ©s\")\n",
    "print(\"\\nğŸ† Meilleur algorithme de chaque type:\")\n",
    "for algo_name, algo_info in best_per_algo.items():\n",
    "    sil = algo_info['silhouette']\n",
    "    print(f\"   â€¢ {algo_name}: Silhouette={sil:.4f if sil else 'N/A'}, Clusters={algo_info['n_clusters']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3d009",
   "metadata": {},
   "source": [
    "## ğŸ“Š MÃ©triques d'Ã©valuation du meilleur algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f52076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les mÃ©triques sont dÃ©jÃ  calculÃ©es pour le meilleur algorithme\n",
    "print(f\"ğŸ“Š MÃ‰TRIQUES DE QUALITÃ‰ - {CLUSTERING_ALGORITHM}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if silhouette is not None:\n",
    "    print(f\"\\nğŸ¯ Silhouette Score: {silhouette:.4f}\")\n",
    "    if silhouette > 0.7:\n",
    "        print(\"   â†’ Excellente sÃ©paration des clusters! â­â­â­\")\n",
    "    elif silhouette > 0.5:\n",
    "        print(\"   â†’ Bonne sÃ©paration des clusters â­â­\")\n",
    "    elif silhouette > 0.3:\n",
    "        print(\"   â†’ SÃ©paration acceptable â­\")\n",
    "    else:\n",
    "        print(\"   â†’ Faible sÃ©paration, essayez d'autres paramÃ¨tres\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‰ Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    if davies_bouldin < 1.0:\n",
    "        print(\"   â†’ Excellente qualitÃ© de clustering! â­â­â­\")\n",
    "    elif davies_bouldin < 1.5:\n",
    "        print(\"   â†’ Bonne qualitÃ© de clustering â­â­\")\n",
    "    else:\n",
    "        print(\"   â†’ QualitÃ© moyenne, essayez d'autres paramÃ¨tres â­\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ Calinski-Harabasz Score: {calinski:.2f}\")\n",
    "    print(\"   â†’ Plus le score est Ã©levÃ©, meilleur est le clustering\")\n",
    "    \n",
    "    # Graphique radar des mÃ©triques normalisÃ©es\n",
    "    print(\"\\nğŸ“Š Comparaison visuelle des 3 mÃ©triques:\")\n",
    "    \n",
    "    # Normaliser les mÃ©triques pour le radar (0-1)\n",
    "    sil_norm = (silhouette + 1) / 2  # De [-1,1] Ã  [0,1]\n",
    "    db_norm = max(0, 1 - davies_bouldin / 3)  # Inverser et normaliser\n",
    "    ch_norm = min(1, calinski / 1000)  # Normaliser (supposant max ~1000)\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=[sil_norm, db_norm, ch_norm, sil_norm],\n",
    "        theta=['Silhouette\\n(SÃ©paration)', 'Davies-Bouldin\\n(CompacitÃ©)', \n",
    "               'Calinski-Harabasz\\n(Variance)', 'Silhouette\\n(SÃ©paration)'],\n",
    "        fill='toself',\n",
    "        name=CLUSTERING_ALGORITHM,\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(visible=True, range=[0, 1])\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        title=f'QualitÃ© du clustering - {CLUSTERING_ALGORITHM}',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Impossible de calculer les mÃ©triques (pas assez de clusters valides)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b397aa",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Visualisation: Scatter Plot 2D/3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a84cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un DataFrame pour Plotly\n",
    "plot_df = pd.DataFrame()\n",
    "plot_df['Cluster'] = [f'Cluster {l}' if l != -1 else 'Bruit' for l in labels]\n",
    "\n",
    "if N_COMPONENTS == 2:\n",
    "    plot_df['Composante 1'] = X_reduced[:, 0]\n",
    "    plot_df['Composante 2'] = X_reduced[:, 1]\n",
    "    \n",
    "    fig = px.scatter(\n",
    "        plot_df,\n",
    "        x='Composante 1',\n",
    "        y='Composante 2',\n",
    "        color='Cluster',\n",
    "        title=f'Visualisation des Clusters - {CLUSTERING_ALGORITHM} + {REDUCTION_METHOD}',\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "        width=900,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white')))\n",
    "    fig.update_layout(\n",
    "        plot_bgcolor='white',\n",
    "        paper_bgcolor='white',\n",
    "        font=dict(size=12),\n",
    "        legend=dict(title='Clusters', orientation='v', yanchor='top', y=1, xanchor='left', x=1.02)\n",
    "    )\n",
    "    \n",
    "elif N_COMPONENTS == 3:\n",
    "    plot_df['Composante 1'] = X_reduced[:, 0]\n",
    "    plot_df['Composante 2'] = X_reduced[:, 1]\n",
    "    plot_df['Composante 3'] = X_reduced[:, 2]\n",
    "    \n",
    "    fig = px.scatter_3d(\n",
    "        plot_df,\n",
    "        x='Composante 1',\n",
    "        y='Composante 2',\n",
    "        z='Composante 3',\n",
    "        color='Cluster',\n",
    "        title=f'Visualisation 3D des Clusters - {CLUSTERING_ALGORITHM} + {REDUCTION_METHOD}',\n",
    "        color_discrete_sequence=px.colors.qualitative.Bold,\n",
    "        width=900,\n",
    "        height=700\n",
    "    )\n",
    "    \n",
    "    fig.update_traces(marker=dict(size=5, opacity=0.7))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7603ec84",
   "metadata": {},
   "source": [
    "## ğŸ“Š Distribution des clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a649cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique en barres\n",
    "unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "cluster_names = [f'Cluster {l}' if l != -1 else 'Bruit' for l in unique_labels]\n",
    "\n",
    "fig = px.bar(\n",
    "    x=cluster_names,\n",
    "    y=counts,\n",
    "    title='Distribution des points par cluster',\n",
    "    labels={'x': 'Cluster', 'y': 'Nombre de points'},\n",
    "    color=counts,\n",
    "    color_continuous_scale='Blues',\n",
    "    text=counts\n",
    ")\n",
    "\n",
    "fig.update_traces(texttemplate='%{text}', textposition='outside')\n",
    "fig.update_layout(\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Tableau des statistiques\n",
    "stats_df = pd.DataFrame({\n",
    "    'Cluster': cluster_names,\n",
    "    'Nombre de points': counts,\n",
    "    'Pourcentage': [f\"{(c/len(labels))*100:.1f}%\" for c in counts]\n",
    "})\n",
    "\n",
    "print(\"\\nğŸ“‹ Tableau de distribution:\")\n",
    "display(stats_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a971d0c",
   "metadata": {},
   "source": [
    "## ğŸ¯ Profils des clusters - Radar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un DataFrame avec les clusters\n",
    "df_with_clusters = df_processed.copy()\n",
    "df_with_clusters['Cluster'] = labels\n",
    "\n",
    "# SÃ©lectionner les features pour le radar chart (max 6 pour la lisibilitÃ©)\n",
    "radar_features = selected_cols[:min(6, len(selected_cols))]\n",
    "\n",
    "# CrÃ©er un radar chart pour chaque cluster (max 4 clusters pour Ã©viter surcharge)\n",
    "unique_labels_sorted = sorted([l for l in set(labels) if l != -1])[:4]\n",
    "\n",
    "for cluster_label in unique_labels_sorted:\n",
    "    cluster_data = df_with_clusters[df_with_clusters['Cluster'] == cluster_label][radar_features]\n",
    "    overall_data = df_processed[radar_features]\n",
    "    \n",
    "    # Calculer les moyennes normalisÃ©es\n",
    "    cluster_means = []\n",
    "    overall_means = []\n",
    "    \n",
    "    for col in radar_features:\n",
    "        cluster_mean = cluster_data[col].mean()\n",
    "        overall_mean = overall_data[col].mean()\n",
    "        overall_std = overall_data[col].std()\n",
    "        \n",
    "        if overall_std > 0:\n",
    "            cluster_normalized = (cluster_mean - overall_mean) / overall_std\n",
    "        else:\n",
    "            cluster_normalized = 0\n",
    "        \n",
    "        cluster_means.append(cluster_normalized)\n",
    "        overall_means.append(0)\n",
    "    \n",
    "    # CrÃ©er le radar chart\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=cluster_means + [cluster_means[0]],\n",
    "        theta=radar_features + [radar_features[0]],\n",
    "        fill='toself',\n",
    "        name=f'Cluster {cluster_label}',\n",
    "        line=dict(color='#1f77b4', width=2)\n",
    "    ))\n",
    "    \n",
    "    fig.add_trace(go.Scatterpolar(\n",
    "        r=overall_means + [overall_means[0]],\n",
    "        theta=radar_features + [radar_features[0]],\n",
    "        fill='toself',\n",
    "        name='Moyenne gÃ©nÃ©rale',\n",
    "        line=dict(dash='dash', color='gray')\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        polar=dict(\n",
    "            radialaxis=dict(visible=True, range=[-3, 3], showticklabels=True)\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        title=f'Profil du Cluster {cluster_label} (Scores Z standardisÃ©s)',\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4eb2a4",
   "metadata": {},
   "source": [
    "## ğŸ”¥ Heatmap des caractÃ©ristiques par cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a980c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les moyennes par cluster pour chaque feature\n",
    "unique_labels_sorted = sorted([l for l in set(labels) if l != -1])\n",
    "\n",
    "cluster_profiles = []\n",
    "for label in unique_labels_sorted:\n",
    "    profile = df_with_clusters[df_with_clusters['Cluster'] == label][selected_cols[:10]].mean()\n",
    "    cluster_profiles.append(profile)\n",
    "\n",
    "if cluster_profiles:\n",
    "    heatmap_data = pd.DataFrame(cluster_profiles)\n",
    "    heatmap_data.index = [f'Cluster {i}' for i in unique_labels_sorted]\n",
    "    \n",
    "    # Normaliser pour la heatmap (scores Z)\n",
    "    heatmap_normalized = (heatmap_data - heatmap_data.mean()) / heatmap_data.std()\n",
    "    \n",
    "    fig = px.imshow(\n",
    "        heatmap_normalized.T,\n",
    "        labels=dict(x=\"Cluster\", y=\"CaractÃ©ristique\", color=\"Score Z\"),\n",
    "        x=heatmap_normalized.index,\n",
    "        y=heatmap_normalized.columns,\n",
    "        color_continuous_scale='RdBu_r',\n",
    "        aspect='auto',\n",
    "        title='Heatmap des profils de clusters (valeurs standardisÃ©es)',\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        font=dict(size=11),\n",
    "        xaxis=dict(tickangle=0)\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"âš ï¸ Pas assez de clusters pour gÃ©nÃ©rer la heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffc4074",
   "metadata": {},
   "source": [
    "## ğŸ¤– Analyse IA - Insights automatiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6e0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ai_insights(df, labels, numeric_cols):\n",
    "    \"\"\"\n",
    "    GÃ©nÃ¨re des insights automatiques sur les clusters\n",
    "    \"\"\"\n",
    "    insights = []\n",
    "    unique_labels = sorted(set(labels))\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            insights.append({\n",
    "                'cluster': 'Bruit/Outliers',\n",
    "                'size': sum(labels == label),\n",
    "                'description': 'Points considÃ©rÃ©s comme du bruit ou des outliers par l\\'algorithme',\n",
    "                'characteristics': []\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        cluster_data = df[labels == label][numeric_cols]\n",
    "        cluster_size = len(cluster_data)\n",
    "        \n",
    "        # Calculer les statistiques\n",
    "        characteristics = []\n",
    "        for col in numeric_cols[:8]:  # Limiter Ã  8 colonnes\n",
    "            mean_val = cluster_data[col].mean()\n",
    "            overall_mean = df[col].mean()\n",
    "            diff_pct = ((mean_val - overall_mean) / overall_mean) * 100 if overall_mean != 0 else 0\n",
    "            \n",
    "            if abs(diff_pct) > 20:  # DiffÃ©rence significative\n",
    "                direction = \"supÃ©rieur\" if diff_pct > 0 else \"infÃ©rieur\"\n",
    "                characteristics.append(f\"{col}: {direction} de {abs(diff_pct):.1f}%\")\n",
    "        \n",
    "        # GÃ©nÃ©rer description\n",
    "        if characteristics:\n",
    "            description = f\"Cluster de {cluster_size} Ã©lÃ©ments avec des caractÃ©ristiques distinctives.\"\n",
    "        else:\n",
    "            description = f\"Cluster de {cluster_size} Ã©lÃ©ments avec un profil proche de la moyenne gÃ©nÃ©rale.\"\n",
    "        \n",
    "        insights.append({\n",
    "            'cluster': f'Cluster {label}',\n",
    "            'size': cluster_size,\n",
    "            'description': description,\n",
    "            'characteristics': characteristics[:5]  # Top 5 caractÃ©ristiques\n",
    "        })\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# GÃ©nÃ©rer les insights\n",
    "insights = generate_ai_insights(df_processed, labels, selected_cols)\n",
    "\n",
    "print(\"ğŸ¤– ANALYSE AUTOMATIQUE PAR IA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for insight in insights:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š {insight['cluster']}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"ğŸ‘¥ Taille: {insight['size']} Ã©lÃ©ments ({insight['size']/len(labels)*100:.1f}%)\")\n",
    "    print(f\"\\nğŸ“ Description:\")\n",
    "    print(f\"   {insight['description']}\")\n",
    "    \n",
    "    if insight['characteristics']:\n",
    "        print(f\"\\nğŸ¯ CaractÃ©ristiques principales:\")\n",
    "        for char in insight['characteristics']:\n",
    "            print(f\"   â€¢ {char}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb236cda",
   "metadata": {},
   "source": [
    "## ğŸ“ RÃ©sumÃ© de l'analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3448ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š RÃ‰SUMÃ‰ DE L'ANALYSE DE CLUSTERING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nğŸ”§ CONFIGURATION\")\n",
    "print(f\"   â€¢ Dataset: {df.shape[0]} lignes Ã— {df.shape[1]} colonnes\")\n",
    "print(f\"   â€¢ Features utilisÃ©es: {len(selected_cols)} colonnes\")\n",
    "print(f\"   â€¢ RÃ©duction de dimensionnalitÃ©: {REDUCTION_METHOD}\")\n",
    "print(f\"   â€¢ Algorithme de clustering: {CLUSTERING_ALGORITHM}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š RÃ‰SULTATS\")\n",
    "print(f\"   â€¢ Nombre de clusters: {len([l for l in set(labels) if l != -1])}\")\n",
    "if -1 in labels:\n",
    "    print(f\"   â€¢ Points de bruit/outliers: {sum(labels == -1)}\")\n",
    "\n",
    "if silhouette is not None:\n",
    "    print(f\"\\nğŸ“ˆ MÃ‰TRIQUES DE QUALITÃ‰\")\n",
    "    print(f\"   â€¢ Silhouette Score: {silhouette:.4f}\")\n",
    "    print(f\"   â€¢ Davies-Bouldin Index: {davies_bouldin:.4f}\")\n",
    "    print(f\"   â€¢ Calinski-Harabasz Score: {calinski:.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ RECOMMANDATIONS\")\n",
    "if silhouette and silhouette > 0.5:\n",
    "    print(\"   âœ… Les clusters sont bien dÃ©finis et distincts\")\n",
    "    print(\"   âœ… La configuration actuelle donne de bons rÃ©sultats\")\n",
    "elif silhouette and silhouette > 0.3:\n",
    "    print(\"   âš ï¸ Les clusters sont acceptables mais pourraient Ãªtre amÃ©liorÃ©s\")\n",
    "    print(\"   ğŸ’¡ Essayez de modifier les hyperparamÃ¨tres ou changez d'algorithme\")\n",
    "else:\n",
    "    print(\"   âš ï¸ La sÃ©paration des clusters est faible\")\n",
    "    print(\"   ğŸ’¡ Recommandations:\")\n",
    "    print(\"      - Essayez un nombre diffÃ©rent de clusters (k)\")\n",
    "    print(\"      - Testez un autre algorithme de clustering\")\n",
    "    print(\"      - SÃ©lectionnez des features plus discriminantes\")\n",
    "    print(\"      - Ajustez les paramÃ¨tres (epsilon, perplexity, etc.)\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa9f8b",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Export des rÃ©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e119aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrÃ©er un DataFrame avec tous les rÃ©sultats\n",
    "results_df = df.copy()\n",
    "results_df['Cluster'] = labels\n",
    "results_df['Cluster_Name'] = [f'Cluster_{l}' if l != -1 else 'Outlier' for l in labels]\n",
    "\n",
    "# Ajouter les composantes rÃ©duites\n",
    "if N_COMPONENTS >= 2:\n",
    "    results_df['Component_1'] = X_reduced[:, 0]\n",
    "    results_df['Component_2'] = X_reduced[:, 1]\n",
    "if N_COMPONENTS == 3:\n",
    "    results_df['Component_3'] = X_reduced[:, 2]\n",
    "\n",
    "# TÃ©lÃ©charger le fichier\n",
    "results_df.to_csv('clustering_results.csv', index=False)\n",
    "files.download('clustering_results.csv')\n",
    "\n",
    "print(\"âœ… RÃ©sultats exportÃ©s dans 'clustering_results.csv'\")\n",
    "print(f\"\\nğŸ“Š Le fichier contient:\")\n",
    "print(f\"   â€¢ Toutes les colonnes originales\")\n",
    "print(f\"   â€¢ Labels des clusters\")\n",
    "print(f\"   â€¢ Composantes rÃ©duites ({N_COMPONENTS}D)\")\n",
    "\n",
    "# Afficher un aperÃ§u\n",
    "print(\"\\nğŸ“‹ AperÃ§u des rÃ©sultats:\")\n",
    "display(results_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39786e3a",
   "metadata": {},
   "source": [
    "## ğŸ“ Conclusion\n",
    "\n",
    "Ce notebook vous a permis de :\n",
    "\n",
    "âœ… Charger et prÃ©traiter vos donnÃ©es automatiquement  \n",
    "âœ… Appliquer plusieurs algorithmes de clustering  \n",
    "âœ… Utiliser diffÃ©rentes techniques de rÃ©duction de dimensionnalitÃ©  \n",
    "âœ… Ã‰valuer la qualitÃ© avec plusieurs mÃ©triques  \n",
    "âœ… Visualiser les rÃ©sultats de maniÃ¨re interactive  \n",
    "âœ… Obtenir des insights automatiques par IA  \n",
    "âœ… Exporter vos rÃ©sultats  \n",
    "\n",
    "### ğŸ’¡ Prochaines Ã©tapes\n",
    "\n",
    "1. **Optimisation**: Testez diffÃ©rents paramÃ¨tres pour amÃ©liorer les mÃ©triques\n",
    "2. **Features Engineering**: CrÃ©ez de nouvelles variables pour mieux capturer les patterns\n",
    "3. **InterprÃ©tation**: Utilisez les insights pour prendre des dÃ©cisions business\n",
    "4. **Validation**: Validez vos clusters avec des experts mÃ©tier\n",
    "\n",
    "---\n",
    "\n",
    "**Challenge 2 : AI Clustering & Insight Dashboard** âœ¨  \n",
    "CrÃ©Ã© pour l'analyse intelligente de donnÃ©es"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
